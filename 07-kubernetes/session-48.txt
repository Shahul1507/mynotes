                                        --------------
                                        | KUBERNETES |  
                                        --------------
                                            DAY-1
                                        -------------		
                                        |  CONTENT  | 
                                        -------------
                                 ---> Need for Kubernetes
                                 ---> Installation using eksctl
                                 ---> K8-Resources
                                      -- namespaces
                                      -- pods 




1. building the images --> Dockerfile
2. running the images --> Containers(docker compose)

we have to create a user and group
all the related mysql directories we should give permissions to this group and user

/var/lib/mysql
/var/run/mysqld
/docker-entrypoint-initdb.d

disadvantages of docker 
------------------------
  -- (there is no relibility) since there is only one docker host...
  -- (there is no autoscaling)
  -- (there is no load balancing)
  -- (volumes are inside docker host)..poor volume management
  -- (security. no secret management)
  -- (no communication between containers in another host.. network management is not good)

--> Because of the disadvantages in docker 
   -- we have "orchestration" in "Kubernetes"
--------------   
orchestration
--------------
--> (In Docker) -- we have "Docker swarn" for (orchestration) but its poor 
    so,we use 
--> (Kubernetes) for flexible orchestration    

kubernetes Install 
-------------------
--> we need;
   
   -- (kubectl) --> k8 client command
   -- (eksctl) --> command to create, update and delete cluster. managing cluster

  STEPS:
  -----   
  1.Docker Install
  -----------------
  {
      --> create instance (workstation) with 50gb volume --> (configure storage)
      --> Install docker in the server (install-docker.sh)
          {
              -- login to server 
              -- curl https://raw.githubusercontent.com/Shahul1507/expense-docker/refs/heads/main/install-docker.sh | sudo bash
          }
      --> re-size the disk (proper partition) 
          {
          
              -- we have added the storage ,yet we need to partition as per our requirement
              -- follow "resize-disk.md" to do the partition
                -- lsblk
                -- sudo growpart /dev/nvme0n1 4 
                -- sudo lvextend -l +50%FREE /dev/RootVG/rootVol
                -- sudo lvextend -l +50%FREE /dev/RootVG/varVol
                -- sudo xfs_growfs /
                -- sudo xfs_growfs /var
              -- check disk memory now: "df -hT"
              -- exit from root access: "exit"
          }
  }
  2.Install kubectl
  -----------------
  {
      --> open-link : https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html#linux_amd64_kubectl
      --> check -- Linux (amd64)
      --> curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.0/2024-09-12/bin/linux/amd64/kubectl
      --> chmod +x ./kubectl
      --> sudo mv kubectl /usr/local/bin/kubectl
      --> kubectl version
  }

  3.Install eksctl
  ---------------- 
  {
      --> open-link: https://eksctl.io/installation/
      --> check -- For unix
      # for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
      --> ARCH=amd64
      --> PLATFORM=$(uname -s)_$ARCH
      --> curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
      # (Optional) Verify checksum
      #curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
      --> tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
      --> sudo mv /tmp/eksctl /usr/local/bin
      --> eksctl version
  }
  4.run aws configure
  ---------------------
  {
      --> aws configure
      --> provide "AWS Access Key ID"
      --> provide "AWS Secret Access Key"
      --> provide "Default region name"
      --> press "enter"
  }

Creating a cluster : (using config-file)
--------------------
{
  -- Ref: https://eksctl.io/getting-started/
  -- Ref: https://eksctl.io/usage/spot-instances/
  -- create a folder "k8-eksctl" in local
  -- create a file "eks.yaml" in "k8-eksctl"
  -- copy the "config-file" to "eks.yaml" 
  -- modify as per our requirement
      --> eks.yaml
         ------------ 
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig

          metadata:
            name: expense-1
            region: us-east-1

          managedNodeGroups:
            - name: expense
              instanceType: m5.large
              desiredCapacity: 3
              spot: true   #reg link : https://eksctl.io/usage/spot-instances/ 

  -- cd k8-ekctl            
  -- git init  
  -- git branch -M main    
  -- git remote add origin https://github.com/Shahul1507/k8-ekctl.git   
  -- git add . ; git commit -m "k8"; git push origin main
  -- git clone https://github.com/Shahul1507/k8-ekctl.git  (in workstation server)
  -- cd k8-eksctl
  -- ls -l
  -- cat eks-yaml
  -- eksctl create cluster --config-file=eks.yaml
  -- kubectl get nodes  #nodes list is displayed

check:
------
-- aws --> cloudformation --> stacks
-- aws --> EKS --> clusters --> "expense-1"
-- aws --> EC2 -->  instances --> "3 nodes will be created"

}

	
spot instances
------------------
AWS have huge data center. there may be unused capacity in data center
spot instances --> 90% discount. when AWS requires capacity to ondemand clients they take back instances with 2min notice...


============
K8-Resources
============

--> namespace -- (just like VPC, you will have a dedicated isolated project to create your workloads/resources)
    ----------
{    
    -- create a folder "k8-resources" in vs-code (local)
    -- create a file "01-namespace.yaml"
    -- check the link (https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/)
    -- copy the namespace file in "01-namespace.yaml"
       --> 01-namespace.yaml
           ------------------
            apiVersion: v1
            kind: Namespace
            metadata:
              name: expense
              labels:
                project: expense
                environment: dev
    (pushing to git-hub)
    -- cd repo/k8-resources 
    -- git init           
    -- git branch -M main
    -- create repo "k8-resources" in git hub
    -- git remote add origin https://github.com/Shahul1507/k8-resources.git
    -- git add . ; git commit -m "k8-resources"; git push origin main
    
    (pulling to workstation)--Mobaxterm
    -- git clone https://github.com/Shahul1507/k8-resources.git  
    -- cd k8-resources
    -- ls -l 
    -- kubectl apply -f 01-namespace.yaml (namespace created)
    -- kubectl get namespaces (shows the list of namespaces)
}
========================================================================================================
--------------------------------------------------------------------------------------------------------
========================================================================================================    

  
--> POD -- (pod is the smallest deployable unit in kubernetes. pod can contain one or many containers.)
   -----
( SINGLE CONTAINER ) 
--------------------  
{
   -- create a file "02-pod.yaml" in "k8-resources" in vs-code (local)
   -- check the link (https://kubernetes.io/docs/concepts/workloads/pods/)
   -- copy the pod file in "02-pod.yaml"
      --> 02-pod.yaml
          ------------
          apiVersion: v1
          kind: Pod
          metadata:
            name: nginx
          spec:
            containers:
            # equivalent to -- docker run -d --name nginx nginx
            - name: nginx
              image: nginx

  (pushing to git-hub)
   -- git add . ; git commit -m "k8-resources"; git push origin main

  (pulling to workstation)--Mobaxterm
   -- git pull
   -- kubectl apply -f 02-pods.yaml (pod created)
   -- kubectl get pods (shows the list of pods)
   -- kubectl exec -it <pod-name> --bash (loging to running pod )
      # curl localhost
      # exit
}      
=========================================================================================================
(MULTI-CONTAINER)
-----------------
{
      -- create a file "03-multi-container.yaml" in "k8-resources" in vs-code (local)
      -- check the link (https://kubernetes.io/docs/concepts/workloads/pods/)
      -- copy the pod file in "03-multi-container.yaml"
         --> 03-multi-container.yaml
             -----------------------
             apiVersion: v1
             kind: Pod
             metadata:
               name: multi-container
             spec:
               containers:
               - name: nginx
                 image: nginx
               - name: almalinux:9
                 image: almalinux:9
                 command: ["sleep","100"]
     (pushing to git-hub)
      -- git add . ; git commit -m "k8-resources"; git push origin main
   
     (pulling to workstation)--Mobaxterm
      -- git pull
      -- kubectl apply -f 03-multi-container.yaml (pod created)
      -- kubectl get pods (CrashLoopBackOff - Error)
      -- change "command: ["sleep","1000"]"
      -- kubectl apply -f 03-multi-container.yaml (we get the error)
      (when there is a change in script "delete & recreate" the pod)
      -- kubectl delete -f 03-multi-container.yaml 
      -- kubectl apply -f 03-multi-container.yaml
      -- kubectl get pods
      -- kubectl exec -it multi-container -c --bash (-c - specific container) (loging to running pod )
         # curl localhost
           --  we see both containers are visible in same pod which are sharong identity and storage  
          
}
=========================================================================================================


Interview:d
----------
Q)  pod vs container ?    
A)  1. pod is the smallest deployable unit in kubernetes
    2. pod can contain one or many containers.
    3. containers in a pod can share same network identity and storage
    4. these are useful in sidecar and proxy patterns
 
Q) How can you login to running pod ? 
A) By using cmd [kubectl exec -it <pod-name> -- bash]

Q) what is "crash loop backoff"?
A) When a container is not able to start ,we get the status of "crashLoopBackOff"
   K8s automatically runs the containers putting to status "RunContainerError"

   -- two pods cannot have same name 
   -- two containers with same name cannot be in a container
   -- containers share same network identity and storage inside a pod 



